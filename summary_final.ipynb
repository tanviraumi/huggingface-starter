{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2490735e-58cc-49fa-90ea-f867938a2cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 16014 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87cf310-6d22-4c1b-836e-e2d5baf9513b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2698d5155d4145869efb214ea9fa5be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before start\n",
      "GPU memory occupied: 283 MB.\n",
      "After loading tokenizer\n",
      "GPU memory occupied: 283 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (/home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ade613057148229e217c967d113ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca24c455dd84ab7b7f4a50963c287ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b509160457242068cd2512616bf33fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a0d4e45b04bcd9f492b28f709370b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loading tokenizer\n",
      "GPU memory occupied: 283 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/tanvir/work/huggingface-starter/pegasus-samsum is already a clone of https://huggingface.co/tanviraumi/pegasus-samsum. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: id, summary, dialogue. If id, summary, dialogue are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training\n",
      "GPU memory occupied: 3460 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230' max='230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230/230 58:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to pegasus-samsum\n",
      "Configuration saved in pegasus-samsum/config.json\n",
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done...Push to hub\n",
      "GPU memory occupied: 16014 MB.\n",
      "Time: 3521.48\n",
      "Samples/second: 4.18\n",
      "GPU memory occupied: 16014 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in pegasus-samsum/pytorch_model.bin\n",
      "tokenizer config file saved in pegasus-samsum/tokenizer_config.json\n",
      "Special tokens file saved in pegasus-samsum/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8481e50609d455d96520479849494b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dda317ecee243cebbdaceafacfbdf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file spiece.model:   2%|1         | 32.0k/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3a9a9ee185455290e0b55ec6fe1afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin: 100%|##########| 2.92k/2.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/tanviraumi/pegasus-samsum\n",
      "   e32a1fe..9792bc5  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Sequence-to-sequence Language Modeling', 'type': 'text2text-generation'}, 'dataset': {'name': 'samsum', 'type': 'samsum', 'args': 'samsum'}}\n",
      "remote: Enforcing permissions...        \n",
      "remote: Allowed refs: all        \n",
      "To https://huggingface.co/tanviraumi/pegasus-samsum\n",
      "   9792bc5..2001c59  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/tanviraumi/pegasus-samsum/commit/9792bc5f7875196d8003e395a4b6421fb5a78f08'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Login to huggingface hub\n",
    "notebook_login()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "print(\"Before start\")\n",
    "print_gpu_utilization()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(\"After loading tokenizer\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length = 1024, truncation = True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length = 128, truncation = True)\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type = \"torch\", columns = columns)\n",
    "print(\"After loading tokenizer\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "# Now we need to create a data collator. This function is called in the Trainer just before the batch is fed through the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'pegasus-samsum', num_train_epochs = 1, warmup_steps = 500, per_device_train_batch_size = 1, per_device_eval_batch_size = 1,\n",
    "    weight_decay = 0.01, logging_steps = 10, push_to_hub = True, evaluation_strategy = 'steps', eval_steps = 500, save_steps = 1e6, gradient_accumulation_steps = 16)\n",
    "trainer = Trainer(model = model, args = training_args, tokenizer = tokenizer, data_collator = seq2seq_data_collator, \n",
    "                 train_dataset = dataset_samsum_pt[\"train\"], eval_dataset = dataset_samsum_pt[\"validation\"])\n",
    "print(\"Before training\")\n",
    "print_gpu_utilization()\n",
    "result = trainer.train()\n",
    "print(\"Training done...Push to hub\")\n",
    "print_gpu_utilization()\n",
    "print_summary(result)\n",
    "trainer.push_to_hub(\"Training Complete\")\n",
    "#score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = \"dialogue\", column_summary = \"summary\")\n",
    "#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "#pd.DataFrame(rouge_dict, index = [\"pegasus\"])\n",
    "\n",
    "# Finally push to hub\n",
    "#trainer.push_to_hub(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e0a565-12a4-4e8b-a74d-801670cd328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410/410 [05:50<00:00,  1.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.398503</td>\n",
       "      <td>0.175022</td>\n",
       "      <td>0.311587</td>\n",
       "      <td>0.311475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.398503  0.175022  0.311587   0.311475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "        \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size = 16, device = device, column_text = \"article\", column_summary = \"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "    \n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total = len(article_batches)):\n",
    "        inputs = tokenizer(article_batch, max_length = 1024, truncation = True, padding = \"max_length\", return_tensors = \"pt\")\n",
    "        summaries = model.generate(input_ids = inputs[\"input_ids\"].to(device), attention_mask = inputs[\"attention_mask\"].to(device), length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens = True, clean_up_tokenization_spaces = True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = \"dialogue\", column_summary = \"summary\")\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index = [\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f138945-90e2-4f49-bf54-06df93b9ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/config.json from cache at /home/tanvir/.cache/huggingface/transformers/6178c373849928347b852c67f88178ee19e69b4e8d2066f00880221675e82ea4.b46a75d1f06c6187e9e1e283bc240896e6772d607bc51a4386380939514cf055\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tanviraumi/pegasus-samsum\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/config.json from cache at /home/tanvir/.cache/huggingface/transformers/6178c373849928347b852c67f88178ee19e69b4e8d2066f00880221675e82ea4.b46a75d1f06c6187e9e1e283bc240896e6772d607bc51a4386380939514cf055\n",
      "Model config PegasusConfig {\n",
      "  \"_name_or_path\": \"tanviraumi/pegasus-samsum\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"PegasusForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 16,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 16,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 1,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"length_penalty\": 0.8,\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 32,\n",
      "  \"model_type\": \"pegasus\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 96103\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/pytorch_model.bin from cache at /home/tanvir/.cache/huggingface/transformers/1f18b3f4a8e2df5dc952fe5fa169d3b7cad52bb0990d0343dc959fd8fed9ddc7.5c6f12816924b9ad933191b8bcadd63d196ade1d633c143c6f50c9b2670da878\n",
      "All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n",
      "\n",
      "All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at tanviraumi/pegasus-samsum.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n",
      "loading file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/spiece.model from cache at /home/tanvir/.cache/huggingface/transformers/974b332dae20e96222192cede96fae89ec8a65523510d711e20673b19cd1f72f.998a78c02c57769973177e597b68eb63a587023acaa1336bbdc49e8c681aaeac\n",
      "loading file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/tokenizer.json from cache at /home/tanvir/.cache/huggingface/transformers/1a5a6f7a141d5e2fd3eca38c7f96486dc1ec85f71d834fdbce3cd61b079e4eb1.1426022f2e186170369cd12cc5573962459a6221f7414abc57680a254d2c33cc\n",
      "loading file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/special_tokens_map.json from cache at /home/tanvir/.cache/huggingface/transformers/de927f922a6b0848f39ed7b40aa48b1b44d438dfbdf1b6d70a627f3e08492ef6.d6a458395976c3c3174927747aca890171a59df352e292371bf46aa0bdd6615d\n",
      "loading file https://huggingface.co/tanviraumi/pegasus-samsum/resolve/main/tokenizer_config.json from cache at /home/tanvir/.cache/huggingface/transformers/3d0e2e68fc75c781d40ac26dd87a7a73509691ea415a91cc353c91b0676c07ef.6bd7e13d2860a6b55ee2dfa8c312bc6105121c296632ce2fe8b8fcba31fe1e57\n",
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Reference Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "\n",
      "Model Summary\n",
      "Amanda can't find Betty's number. Larry called Betty last time they were at the park together. He's very nice. Hannah would rather she text him.\n"
     ]
    }
   ],
   "source": [
    "# Lets see how our newly trained model performs against a sample\n",
    "from transformers import pipeline\n",
    "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\": 8, \"max_length\": 128}\n",
    "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
    "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
    "pipe = pipeline(\"summarization\", model = \"tanviraumi/pegasus-samsum\")\n",
    "\n",
    "print(\"Dialogue:\")\n",
    "print(sample_text)\n",
    "print(\"\\nReference Summary:\")\n",
    "print(reference)\n",
    "print(\"\\nModel Summary\")\n",
    "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4bf6d55-633a-4bd9-b435-462575dacfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highspot is a company that provides sales enablers. It helps your sales people to perform sales efficiently. Kurt can do it better, Steve can do it better.\n"
     ]
    }
   ],
   "source": [
    "custom_conversation = \"\"\"\\\n",
    "Tanvir: Hi guys, have you heard of Highspot or sales enablement?\n",
    "Kurt: No, explain it to me\n",
    "Tanvir: Highspot is a company that provides sales enablement. It helps your sales people to perform sales efficiently.\n",
    "Kurt: Tell me more\n",
    "Tanvir: Steve can do it better\n",
    "Steve: HIghpot does sales enablement by providing a content aggregation platform along with training, coaching, and engagement tools. For sales enablement, Highspot is all you need. \n",
    "\"\"\"\n",
    "print(pipe(custom_conversation, **gen_kwargs)[0][\"summary_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab2e87-459c-43fc-b433-9ffb8289db34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
