{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8ae82bbe-7f8f-4b2f-8e73-8309e3ecdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun experiment generating slogan using GPT-2: https://jonathanbgn.com/gpt2/2020/01/20/slogan-generator.html\n",
    "# Dataset is download from https://github.com/jonathanbgn/slogan-generator/blob/master/slogans.csv\n",
    "\n",
    "# Load GPT-2\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "MODEL_NAME = 'distilgpt2' #'distilgpt2' 'gpt2-medium'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96b78aaf-19c2-4f0a-99a8-a062d3b0e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<context>', '<slogan>']}\n"
     ]
    }
   ],
   "source": [
    "# The dataset is a fairly simple csv file: company,slogan. Our model need to understand the difference between\n",
    "# compnay and slogan. For this purpose, we introduce two special delimiter tokens: <context> and <slogan>. In addition,\n",
    "# we also need a padding token.\n",
    "\n",
    "SPECIAL_TOKENS_DICT = {\n",
    "    'pad_token': '<pad>',\n",
    "    'additional_special_tokens': ['<context>', '<slogan>'],\n",
    "}\n",
    "\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Show the full list of special tokens:\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "48d3161f-f946-43b6-a4f3-fbbdd059371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "# Now we need to prepare data. For each slogan, we will need to create 3 sequences as input for our model:\n",
    "# 1. The context and the slogan delimitated by <slogan> and <context> (as described above)\n",
    "# 2. The “token type ids” sequence, annotating each token to the context or slogan segment\n",
    "# 3. The label tokens, representing the ground truth and used to compute the cost function\n",
    "\n",
    "# cross-entropy is our cost function to minimize. However, we do not want to penalize our model for what it is not supposed to predict (the context and the padding tokens),\n",
    "# hence we will only compute cross-entropy on the slogan’s tokens. Using the Transformer library, setting label ids to -1 will tag them to be ignored during cross-entropy computation.\n",
    "\n",
    "# We will create our own class to load and preprocess the data from the CSV file, based on PyTorch’s base Dataset class:\n",
    "\n",
    "import csv, torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SloganDataset(Dataset):\n",
    "  def __init__(self, filename, tokenizer, seq_length=64):\n",
    "\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "    pad_tkn = tokenizer.pad_token_id\n",
    "    eos_tkn = tokenizer.eos_token_id\n",
    "\n",
    "    self.examples = []\n",
    "    with open(filename) as csvfile:\n",
    "      reader = csv.reader(csvfile)\n",
    "      for row in reader:\n",
    "\n",
    "        # Build the context and slogan segments:\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\n",
    "\n",
    "        # Concatenate the two parts together:\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Annotate each token with its corresponding segment:\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )\n",
    "\n",
    "        # Ignore the context, padding, and <slogan> tokens by setting their labels to -1\n",
    "        labels = [-100] * (len(context)+1) + slogan[1:] + [-100] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Add the preprocessed example to the dataset\n",
    "        self.examples.append((tokens, segments, labels))\n",
    "        #print(len(tokens))\n",
    "        #print(len(segments))\n",
    "        #print(len(labels))\n",
    "        #print()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item])\n",
    "\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\n",
    "slogan_dataset = SloganDataset('slogans.csv', tokenizer)\n",
    "print(next(iter(slogan_dataset)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "044fbf57-1856-481a-8975-e9b10254aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we divide this dataset into 2 DataLoaders for training and validation.\n",
    "# Note: we can double the batch size for validation since no backprogation is involved (thus it should fit on the GPU’s memory).\n",
    "import math, random\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Create data indices for training and validation splits:\n",
    "indices = list(range(len(slogan_dataset)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = math.floor(0.1 * len(slogan_dataset))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Build the PyTorch data loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8f3d2c5-00ed-4b73-980d-9cad8bf240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will need to train our model and keep track of cross-entropy for both the training and validation set.\n",
    "# The following is a function that will perform back-propagation and validation for the number of epochs specified.\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print('\\n--- Starting epoch #{} ---'.format(i))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n",
    "    losses = []\n",
    "    nums = []\n",
    "\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
    "      # Move the batch to the training device:\n",
    "      inputs = xb.to(device)\n",
    "\n",
    "      # Call the model with the token ids, segment ids, and the ground truth (labels)\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "\n",
    "      # Add the loss and batch size to the list:\n",
    "      loss = outputs[0]\n",
    "      losses.append(loss.item())\n",
    "      nums.append(len(xb))\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "    # Compute the average cost over one epoch:\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "\n",
    "    # Now do the same thing for validation:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      losses = []\n",
    "      nums = []\n",
    "\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
    "        inputs = xb.to(device)\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "        losses.append(outputs[0].item())\n",
    "        nums.append(len(xb))\n",
    "\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87c50746-dc1f-4cb2-b095-7637c4da0d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting epoch #0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 268/268 [10:26<00:00,  2.34s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:18<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #0 finished --- Training cost: 4.397268406594612 / Validation cost: 3.2270585148274398\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 268/268 [10:30<00:00,  2.35s/it]\n",
      "Validation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:18<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 2.6939699883554495 / Validation cost: 3.287536739301281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We only run for 2 epoch so that the model does not overfit our small dataset.\n",
    "from transformers import AdamW\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Move the model to the GPU:\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune GPT2 for two epochs:\n",
    "optimizer = AdamW(model.parameters())\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b4ed327-56e9-40ad-8f52-db9029af8065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_37', 'sm_50', 'sm_60', 'sm_70', 'sm_75', 'sm_80', 'sm_86']\n",
      "11.3\n",
      "1.13.0.dev20220914+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "# print(len(slogan_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6be11865-62a7-4492-ab1d-2cf819632d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling functions with top k and top p from HuggingFace:\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "# From HuggingFace, adapted to work with the context/slogan separation:\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if segments_tokens != None:\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
    "\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cc5c7dc9-d4ac-4ab1-aea9-f39cda8195f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:02<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " The coffee chain. Any time. AnyStarbucks. Anywhere.\n",
      " The coffee chain.\n",
      " Winter water from the air.\n",
      " The place you live in Seattle.\n",
      " Here & anywhere.\n",
      " There's a way to spend happy lives.\n",
      " Great coffee is hot fire.\n",
      " You're America's Starbucks.\n",
      " Blinking coffee beans twice a month.\n",
      " Where coffee lovers meet.\n",
      " Starbucks. Taste your home.\n",
      " Put a littleCalif style on every coffee chain\n",
      " Have a great day out.\n",
      " Triumph in coffee.\n",
      " Experience. Experience.\n",
      " starbucks. We're fresher.\n",
      " Darewell never chilled.\n",
      " Starbucks. Nothing's news you can say.\n",
      " West. Food for everyone.\n",
      " Alive with style.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context = \"Starbucks, coffee chain from Seattle\"\n",
    "\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\n",
    "\n",
    "segments = [slogan_tkn] * 64\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
    "\n",
    "input_ids += [slogan_tkn]\n",
    "\n",
    "# Move the model back to the CPU for inference:\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# Generate 20 samples of max length 20\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\n",
    "\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\n",
    "\n",
    "for g in generated:\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
    "  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\n",
    "  print(slogan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020061e6-4437-4163-b820-efc13c2e5e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
