{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40a79059-3ffc-4980-9660-7572ce526938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset cnn_dailymail (/home/tanvir/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0831885e4664566ab97eef248aaf85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['article', 'highlights', 'id']\n",
      "\n",
      "Article: total length: 3192\n",
      "\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n"
     ]
    }
   ],
   "source": [
    "# Load the CNN dailymail dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version = \"3.0.0\")\n",
    "print(f\"Features: {dataset['train'].column_names}\")\n",
    "\n",
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article: total length: {len(sample[\"article\"])}\n",
    "\"\"\")\n",
    "\n",
    "print(sample[\"article\"][:500])\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0350b3f1-b9d8-4633-a368-2e4b5efec1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tanvir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We will now compare different models with the sample_text\n",
    "sample_text = dataset[\"train\"][1][\"article\"][:2000]\n",
    "# We will collect summaries in the following dictionary\n",
    "summaries = {}\n",
    "\n",
    "# The usual convention is to split texts with a newline. Lets load a sophisticated algo to do that\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "string = \"The U.S. are a country. The U.N. is an organization\"\n",
    "sent_tokenize(string)\n",
    "\n",
    "# The baseline is just first three sentences of the article\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "summaries[\"baseline\"] = three_sentence_summary(sample_text)\n",
    "print(summaries[\"baseline\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a871dee-6e0b-491c-88ab-8244efd95576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# GPT-2: You just need to add TL;DR after the article an GPT-2 will generate a summary!\n",
    "from transformers import pipeline, set_seed\n",
    "set_seed(42)\n",
    "pipe = pipeline(\"text-generation\", model = \"gpt2-xl\")\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "pipe_out = pipe(gpt2_query, max_length = 512, clean_up_tokenization_spaces = True)\n",
    "summaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "489a4f41-9bed-4f2d-8c6a-cbfb50a572a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"t5-large\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c27769e-67f7-4c88-a2b4-35960c407abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "261ead3c-0d28-4d84-8aa1-fba5b1eca0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model = \"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(sample_text)\n",
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f2a5e90-3f36-407a-aa1a-5d6260332cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Usain Bolt wins third gold of world championship .\n",
      "Anchors Jamaica to 4x100m relay victory .\n",
      "Eighth gold at the championships for Bolt .\n",
      "Jamaica double up in women's 4x100m relay .\n",
      "\n",
      "BASELINE\n",
      "(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\n",
      "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
      "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
      "\n",
      "GPT2\n",
      "Nesta, the fastest man in the world.\n",
      "Gatlin, the most successful Olympian ever.\n",
      "Kemar, a Jamaican legend.\n",
      "Shelly-Ann, the fastest woman ever.\n",
      "Bolt, the world's greatest athlete.\n",
      "The team sport of pole vaulting\n",
      "\n",
      "T5\n",
      "usain bolt wins his third gold medal of the world championships in the men's 4x100m relay .\n",
      "the 26-year-old anchored Jamaica to victory in the event in the Russian capital .\n",
      "he has now collected eight gold medals at the championships, equaling the record .\n",
      "\n",
      "BART\n",
      "Usain Bolt wins his third gold of the world championships in Moscow.\n",
      "Bolt anchors Jamaica to victory in the men's 4x100m relay.\n",
      "The 26-year-old has now won eight gold medals at world championships.\n",
      "Jamaica's women also win gold in the relay, beating France in the process.\n",
      "\n",
      "PEGASUS\n",
      "Usain Bolt wins third gold of world championships.\n",
      "Anchors Jamaica to victory in men's 4x100m relay.\n",
      "Eighth gold at the championships for Bolt.\n",
      "Jamaica also win women's 4x100m relay .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now its time to compare the output from different models\n",
    "print(\"GROUND TRUTH\")\n",
    "print(dataset[\"train\"][1][\"highlights\"])\n",
    "print(\"\")\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95b1f9fb-9f84-4589-93bb-488a671289f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/tanvir/.cache/huggingface/datasets/cnn_dailymail/default/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-e3d7667669a9eaad.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.388071</td>\n",
       "      <td>0.170554</td>\n",
       "      <td>0.247146</td>\n",
       "      <td>0.354972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.388071  0.170554  0.247146   0.354972"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we need to measure the quality of summaries with some metrics. We will use Rouge since its more suited for summarization tasks\n",
    "from datasets import load_metric\n",
    "import pandas as pd\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "reference = dataset[\"train\"][1][\"highlights\"]\n",
    "records = []\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction = summaries[model_name], reference = reference)\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "    records.append(rouge_dict)\n",
    "pd.DataFrame.from_records(records, index = summaries.keys())\n",
    "\n",
    "# Lets write a function to evaluate the performance of baseline summaries and run it over the samples.\n",
    "def evaluate_summaries_baseline(dataset, metric, column_text = \"article\", column_summary = \"highlights\"):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n",
    "    metric.add_batch(predictions = summaries, references = dataset[column_summary])\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "# To make everything faster, we run evaluations on a sample of 1000.\n",
    "test_sampled = dataset[\"test\"].shuffle(seed = 42).select(range(1000))\n",
    "score = evaluate_summaries_baseline(test_sampled, rouge_metric)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame.from_dict(rouge_dict, orient = \"index\", columns = [\"baseline\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78aeb477-3024-40ed-8a87-7b68a7e6b753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 125/125 [12:25<00:00,  5.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.427255</td>\n",
       "      <td>0.207432</td>\n",
       "      <td>0.305332</td>\n",
       "      <td>0.369296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.427255  0.207432  0.305332   0.369296"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets implement the same function for evaluating pegasus model\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def chunks(list_of_elements, batch_size):\n",
    "    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "        \n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size = 16, device = device, column_text = \"article\", column_summary = \"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "    \n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total = len(article_batches)):\n",
    "        inputs = tokenizer(article_batch, max_length = 1024, truncation = True, padding = \"max_length\", return_tensors = \"pt\")\n",
    "        summaries = model.generate(input_ids = inputs[\"input_ids\"].to(device), attention_mask = inputs[\"attention_mask\"].to(device), length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens = True, clean_up_tokenization_spaces = True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "    \n",
    "\n",
    "# Now lets load the model again and evaluate it\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "score = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size = 8)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index = [\"pegasus\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db1f0cc2-d8b9-43f5-a8b8-3809fd4d6aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (/home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0a5a3ea82042ff95cf223bfaecf2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split lengths : [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogues:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him 🙂\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 122. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary with Pegasus:\n",
      "Amanda: Ask Larry Amanda: He called her last time we were at the park together.\n",
      "Hannah: I'd rather you texted him.\n",
      "Amanda: Just text him .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 103/103 [09:09<00:00,  5.33s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.295856</td>\n",
       "      <td>0.087267</td>\n",
       "      <td>0.228962</td>\n",
       "      <td>0.229399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           rouge1    rouge2    rougeL  rougeLsum\n",
       "pegasus  0.295856  0.087267  0.228962   0.229399"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now its time to train our model on the SAMsum datatset. First lets insepct the dataset a little\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n",
    "\n",
    "print(f\"Split lengths : {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogues:\")\n",
    "print(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][0][\"summary\"])\n",
    "\n",
    "# Lets run the pegasus model on the dataset and see how it goes\n",
    "pipe = pipeline(\"summarization\", model = \"google/pegasus-cnn_dailymail\")\n",
    "pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\n",
    "print(\"Summary with Pegasus:\")\n",
    "print(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))\n",
    "\n",
    "# The summary oes not look very nice. Lets confirm that by running Rouge evaluation\n",
    "score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text = \"dialogue\", column_summary = \"summary\", batch_size = 8)\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "pd.DataFrame(rouge_dict, index = [\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "745b3629-01e6-48ff-9c9e-4e29af03367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x252 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now its time to fine-tune the data. Lets take a quick look at the length distribution of inputs and outputs\n",
    "import matplotlib.pyplot as plt\n",
    "d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\n",
    "s_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 3.5), sharey = True)\n",
    "axes[0].hist(d_len, bins = 20, color = \"C0\", edgecolor = \"C0\")\n",
    "axes[0].set_title(\"Dialogue Token Length\")\n",
    "axes[0].set_xlabel(\"Length\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[1].hist(d_len, bins = 20, color = \"C0\", edgecolor = \"C0\")\n",
    "axes[1].set_title(\"Summary Token Length\")\n",
    "axes[1].set_xlabel(\"Length\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d9db00-ce2b-48a8-bcc2-44da4667edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (/home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0325f1ccbe304fff8133cdfb4d7e6c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6/cache-9ac948daec4efa06.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afa708c1a4244e8ba9c849d60705c0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6/cache-0a8e1695c85dcf65.arrow\n"
     ]
    }
   ],
   "source": [
    "# Based on the distribution, We will set 1024 and 128 for dialogues and summaries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length = 1024, truncation = True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length = 128, truncation = True)\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type = \"torch\", columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee39d1ab-9072-4380-befc-f9e1f648e8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset samsum (/home/tanvir/.cache/huggingface/datasets/samsum/samsum/0.0.0/3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39bd31595cc4f208eafe03a6aa365a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a558bc33eb345468b5c36cd9a644ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4636041d81c544d9933fcd70cb23e4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9959ce13997c4406924f956a7b6a8040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c74c24894384c41a80f437a538346c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1402: UserWarning: positional arguments and argument \"destination\" are deprecated. nn.Module.state_dict will not accept them in the future. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\n",
      "/home/tanvir/work/huggingface-starter/pegasus-samsum is already a clone of https://huggingface.co/tanviraumi/pegasus-samsum. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "The following columns in the training set  don't have a corresponding argument in `PegasusForConditionalGeneration.forward` and have been ignored: summary, id, dialogue. If summary, id, dialogue are not expected by `PegasusForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14732\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanvir/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.74 GiB total capacity; 14.42 GiB already allocated; 15.94 MiB free; 14.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model \u001b[38;5;241m=\u001b[39m model, args \u001b[38;5;241m=\u001b[39m training_args, tokenizer \u001b[38;5;241m=\u001b[39m tokenizer, data_collator \u001b[38;5;241m=\u001b[39m seq2seq_data_collator, \n\u001b[1;32m     52\u001b[0m                  train_dataset \u001b[38;5;241m=\u001b[39m dataset_samsum_pt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], eval_dataset \u001b[38;5;241m=\u001b[39m dataset_samsum_pt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining done...Push to hub\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/transformers/trainer.py:1422\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1422\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1425\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1428\u001b[0m ):\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/transformers/trainer.py:2029\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2028\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2029\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/_tensor.py:399\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    392\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    393\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    398\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 399\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:146\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs_with_grad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone of output has requires_grad=True,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m this checkpoint() is not necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs_with_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_with_grad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inp\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    148\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m detached_inputs)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m+\u001b[39m grads\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/autograd/function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    252\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:34\u001b[0m, in \u001b[0;36mBroadcast.backward\u001b[0;34m(ctx, *grad_outputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39mgrad_outputs):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m+\u001b[39m \u001b[43mReduceAddCoalesced\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:45\u001b[0m, in \u001b[0;36mReduceAddCoalesced.forward\u001b[0;34m(ctx, destination, num_inputs, *grads)\u001b[0m\n\u001b[1;32m     41\u001b[0m ctx\u001b[38;5;241m.\u001b[39mtarget_gpus \u001b[38;5;241m=\u001b[39m [grads[i]\u001b[38;5;241m.\u001b[39mget_device() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)]\n\u001b[1;32m     43\u001b[0m grads_ \u001b[38;5;241m=\u001b[39m [grads[i:i \u001b[38;5;241m+\u001b[39m num_inputs]\n\u001b[1;32m     44\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(grads), num_inputs)]\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_add_coalesced\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/comm.py:143\u001b[0m, in \u001b[0;36mreduce_add_coalesced\u001b[0;34m(inputs, destination, buffer_size)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunks \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mitrs):\n\u001b[1;32m    142\u001b[0m     flat_tensors \u001b[38;5;241m=\u001b[39m [_flatten_dense_tensors(chunk) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]  \u001b[38;5;66;03m# (num_gpus,)\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     flat_result \u001b[38;5;241m=\u001b[39m \u001b[43mreduce_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m _unflatten_dense_tensors(flat_result, chunks[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;66;03m# The unflattened tensors do not share storage, and we don't expose\u001b[39;00m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;66;03m# base flat tensor anyways, so give them different version counters.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;66;03m# See NOTE [ Version Counter in comm.*_coalesced ]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(t\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/work/huggingface-starter/.env/lib/python3.10/site-packages/torch/nn/parallel/comm.py:95\u001b[0m, in \u001b[0;36mreduce_add\u001b[0;34m(inputs, destination)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nccl\u001b[38;5;241m.\u001b[39mis_available(inputs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroot_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     nccl\u001b[38;5;241m.\u001b[39mreduce(inputs, output\u001b[38;5;241m=\u001b[39mresult, root\u001b[38;5;241m=\u001b[39mroot_index)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 15.74 GiB total capacity; 14.42 GiB already allocated; 15.94 MiB free; 14.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from huggingface_hub import notebook_login\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "#import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length = 1024, truncation = True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch[\"summary\"], max_length = 128, truncation = True)\n",
    "    return {\"input_ids\": input_encodings[\"input_ids\"],\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"],\n",
    "            \"labels\": target_encodings[\"input_ids\"]}\n",
    "\n",
    "def evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size = 16, device = device, column_text = \"article\", column_summary = \"highlights\"):\n",
    "    article_batches = list(chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(chunks(dataset[column_summary], batch_size))\n",
    "    \n",
    "    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total = len(article_batches)):\n",
    "        inputs = tokenizer(article_batch, max_length = 1024, truncation = True, padding = \"max_length\", return_tensors = \"pt\")\n",
    "        summaries = model.generate(input_ids = inputs[\"input_ids\"].to(device), attention_mask = inputs[\"attention_mask\"].to(device), length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens = True, clean_up_tokenization_spaces = True) for s in summaries]\n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        metric.add_batch(predictions = decoded_summaries, references = target_batch)\n",
    "    score = metric.compute()\n",
    "    return score\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\")\n",
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n",
    "columns = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "dataset_samsum_pt.set_format(type = \"torch\", columns = columns)\n",
    "\n",
    "# Login to huggingface hub\n",
    "notebook_login()\n",
    "\n",
    "# Now we need to create a data collator. This function is called in the Trainer just before the batch is fed through the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'pegasus-samsum', num_train_epochs = 1, warmup_steps = 500, per_device_train_batch_size = 1, per_device_eval_batch_size = 1,\n",
    "    weight_decay = 0.01, logging_steps = 10, push_to_hub = True, evaluation_strategy = 'steps', eval_steps = 500, save_steps = 1e6, gradient_accumulation_steps = 16, gradient_checkpointing=True)\n",
    "trainer = Trainer(model = model, args = training_args, tokenizer = tokenizer, data_collator = seq2seq_data_collator, \n",
    "                 train_dataset = dataset_samsum_pt[\"train\"], eval_dataset = dataset_samsum_pt[\"validation\"])\n",
    "print(\"Started training\")\n",
    "trainer.train()\n",
    "print(\"Training done...Push to hub\")\n",
    "trainer.push_to_hub(\"Training Complete\")\n",
    "#trainer.push_to_hub(\"Training Complete\")\n",
    "#score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = \"dialogue\", column_summary = \"summary\")\n",
    "#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "#pd.DataFrame(rouge_dict, index = [\"pegasus\"])\n",
    "\n",
    "# Finally push to hub\n",
    "#trainer.push_to_hub(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37f7a691-9b09-4325-83d7-6e1e5def6954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 16360 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n",
    "print_gpu_utilization()  \n",
    "\n",
    "custom_conversation = \"\"\"\\\n",
    "Tanvir: Hi guys, have you heard of Highspot or sales enablement?\n",
    "Kurt: No, explain it to me\n",
    "Tanvir: Highspot is a company that provides sales enablement. It helps your sales people to perform sales efficiently.\n",
    "Kurt: Tell me more\n",
    "Tanvir: Steve can do it better\n",
    "Steve: HIghpot does sales enablement by providing a content aggregation platform along with training, coaching, and engagement tools. For sales enablement, Highspot is all you need. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282825-86ce-49ed-bbfb-df1e3eb4cd53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb7dd1-36d8-45ab-9cc2-ae35a3ee8608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
